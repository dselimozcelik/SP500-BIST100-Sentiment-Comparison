# src/sentiment_price_analysis.py
"""
Sentiment ve Fiyat Verilerini BirleÅŸtirip Analiz Eden ModÃ¼l
Korelasyon, regresyon ve tahmin analizi yapar.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit
from sklearn.metrics import (accuracy_score, classification_report, 
                             confusion_matrix, mean_squared_error, r2_score)
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
from scipy import stats
import os
import warnings
warnings.filterwarnings('ignore')

# --- AYARLAR ---
DATA_DIR = "data"
RESULTS_DIR = "results"

# Matplotlib stil
plt.style.use('seaborn-v0_8-darkgrid')
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 11


class SentimentPriceAnalyzer:
    """
    Sentiment ve fiyat verilerini birleÅŸtirip analiz eden sÄ±nÄ±f.
    """
    
    def __init__(self):
        self.merged_data = None
        self.model_results = {}
        
        # SonuÃ§ dizinini oluÅŸtur
        os.makedirs(RESULTS_DIR, exist_ok=True)
    
    def load_and_merge_data(self, 
                            sentiment_file: str = None,
                            price_file: str = None) -> pd.DataFrame:
        """
        Sentiment ve fiyat verilerini yÃ¼kleyip birleÅŸtirir.
        """
        if sentiment_file is None:
            sentiment_file = os.path.join(DATA_DIR, "daily_sentiment.csv")
        if price_file is None:
            price_file = os.path.join(DATA_DIR, "bist100_prices.csv")
        
        print("ðŸ“‚ Veriler yÃ¼kleniyor...")
        
        # Sentiment verisi
        if os.path.exists(sentiment_file):
            sentiment_df = pd.read_csv(sentiment_file)
            sentiment_df['date'] = pd.to_datetime(sentiment_df['date']).dt.date
            print(f"   âœ… Sentiment: {len(sentiment_df)} gÃ¼n")
        else:
            print(f"   âŒ Sentiment dosyasÄ± bulunamadÄ±: {sentiment_file}")
            return pd.DataFrame()
        
        # Fiyat verisi
        if os.path.exists(price_file):
            price_df = pd.read_csv(price_file)
            price_df['Date'] = pd.to_datetime(price_df['Date']).dt.date
            print(f"   âœ… Fiyat: {len(price_df)} gÃ¼n")
        else:
            print(f"   âŒ Fiyat dosyasÄ± bulunamadÄ±: {price_file}")
            return pd.DataFrame()
        
        # BirleÅŸtir (inner join - sadece her iki veri setinde de olan gÃ¼nler)
        merged = pd.merge(
            sentiment_df, 
            price_df,
            left_on='date',
            right_on='Date',
            how='inner'
        )
        
        # Gereksiz sÃ¼tunlarÄ± kaldÄ±r
        if 'Date' in merged.columns:
            merged = merged.drop(columns=['Date'])
        
        print(f"\nðŸ”— BirleÅŸtirildi: {len(merged)} ortak gÃ¼n")
        
        # Lag deÄŸiÅŸkenleri ekle (geÃ§miÅŸ sentiment'in etkisi)
        merged = merged.sort_values('date')
        merged['sentiment_lag1'] = merged['avg_sentiment'].shift(1)
        merged['sentiment_lag2'] = merged['avg_sentiment'].shift(2)
        merged['sentiment_lag3'] = merged['avg_sentiment'].shift(3)
        
        # KÃ¼mÃ¼latif sentiment (son 5 gÃ¼n)
        merged['sentiment_ma5'] = merged['avg_sentiment'].rolling(window=5).mean()
        
        # Sentiment momentum (deÄŸiÅŸim)
        merged['sentiment_change'] = merged['avg_sentiment'].diff()
        
        # NaN'larÄ± temizle
        merged = merged.dropna()
        
        self.merged_data = merged
        print(f"   Son veri: {len(merged)} satÄ±r (lag sonrasÄ±)")
        
        return merged
    
    def correlation_analysis(self) -> dict:
        """
        Sentiment ve fiyat arasÄ±ndaki korelasyonu analiz eder.
        """
        if self.merged_data is None:
            print("âŒ Ã–nce load_and_merge_data() Ã§alÄ±ÅŸtÄ±rÄ±n!")
            return {}
        
        print("\n" + "=" * 60)
        print("ðŸ“Š KORELASYON ANALÄ°ZÄ°")
        print("=" * 60)
        
        df = self.merged_data
        
        # Korelasyon matrisi
        corr_cols = ['avg_sentiment', 'sentiment_lag1', 'sentiment_ma5',
                     'Daily_Return', 'Next_Day_Return', 'Close', 'Volume']
        
        # Mevcut sÃ¼tunlarÄ± filtrele
        corr_cols = [c for c in corr_cols if c in df.columns]
        
        corr_matrix = df[corr_cols].corr()
        
        # GÃ¶rselleÅŸtir
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(corr_matrix, annot=True, cmap='RdYlGn', center=0,
                    fmt='.3f', ax=ax, vmin=-1, vmax=1)
        ax.set_title('Sentiment vs Fiyat Korelasyon Matrisi', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(os.path.join(RESULTS_DIR, 'correlation_matrix.png'), dpi=150)
        plt.close()
        
        # Ã–nemli korelasyonlar
        results = {
            'sentiment_vs_same_day_return': df['avg_sentiment'].corr(df['Daily_Return']),
            'sentiment_vs_next_day_return': df['avg_sentiment'].corr(df['Next_Day_Return']),
            'lagged_sentiment_vs_return': df['sentiment_lag1'].corr(df['Daily_Return']),
            'sentiment_ma5_vs_return': df['sentiment_ma5'].corr(df['Daily_Return'])
        }
        
        print("\nðŸ” Ã–nemli Korelasyonlar:")
        for name, value in results.items():
            significance = "***" if abs(value) > 0.1 else "**" if abs(value) > 0.05 else "*" if abs(value) > 0.02 else ""
            print(f"   {name}: {value:.4f} {significance}")
        
        # Ä°statistiksel anlamlÄ±lÄ±k testi
        print("\nðŸ“ Ä°statistiksel AnlamlÄ±lÄ±k (Pearson):")
        stat, p_value = stats.pearsonr(df['avg_sentiment'], df['Next_Day_Return'])
        print(f"   Sentiment -> Ertesi GÃ¼n Getiri: r={stat:.4f}, p={p_value:.4f}")
        print(f"   AnlamlÄ± mÄ±? {'Evet âœ…' if p_value < 0.05 else 'HayÄ±r âŒ'} (p < 0.05)")
        
        self.model_results['correlation'] = results
        return results
    
    def granger_causality_test(self, max_lag: int = 5) -> dict:
        """
        Granger nedensellik testi - sentiment fiyatÄ± tahmin ediyor mu?
        """
        if self.merged_data is None:
            return {}
        
        print("\n" + "=" * 60)
        print("ðŸ“Š GRANGER NEDENSELLÄ°K TESTÄ°")
        print("=" * 60)
        
        from statsmodels.tsa.stattools import grangercausalitytests
        
        df = self.merged_data[['avg_sentiment', 'Daily_Return']].dropna()
        
        print(f"\nH0: Sentiment, Getiriyi Granger-nedenlemez")
        print(f"Test edilen lag sayÄ±sÄ±: 1-{max_lag}")
        
        try:
            results = grangercausalitytests(df[['Daily_Return', 'avg_sentiment']], 
                                           maxlag=max_lag, verbose=False)
            
            print("\n   Lag | F-test p-deÄŸeri | SonuÃ§")
            print("   " + "-" * 40)
            
            granger_results = {}
            for lag in range(1, max_lag + 1):
                p_value = results[lag][0]['ssr_ftest'][1]
                significant = "âœ… AnlamlÄ±" if p_value < 0.05 else "âŒ AnlamsÄ±z"
                print(f"   {lag:3d} | {p_value:.4f}          | {significant}")
                granger_results[f'lag_{lag}'] = p_value
            
            self.model_results['granger'] = granger_results
            return granger_results
            
        except Exception as e:
            print(f"âŒ Granger testi hatasÄ±: {e}")
            return {}
    
    def train_prediction_model(self, target: str = 'Next_Day_Direction') -> dict:
        """
        Sentiment'ten fiyat yÃ¶nÃ¼ tahmini iÃ§in ML modeli eÄŸitir.
        
        Args:
            target: 'Next_Day_Direction' (binary) veya 'Next_Day_Return' (continuous)
        """
        if self.merged_data is None:
            return {}
        
        print("\n" + "=" * 60)
        print("ðŸ¤– TAHMÄ°N MODELÄ° EÄžÄ°TÄ°MÄ°")
        print("=" * 60)
        
        df = self.merged_data.dropna()
        
        # Ã–zellikler
        feature_cols = ['avg_sentiment', 'sentiment_lag1', 'sentiment_lag2',
                        'sentiment_ma5', 'sentiment_change', 'news_count',
                        'sentiment_std', 'Volatility_20d']
        
        # Mevcut sÃ¼tunlarÄ± filtrele
        feature_cols = [c for c in feature_cols if c in df.columns]
        
        X = df[feature_cols]
        y = df[target]
        
        # Veriyi bÃ¶l (zaman serisi iÃ§in shuffle=False)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, shuffle=False  # Zaman sÄ±rasÄ±nÄ± koru!
        )
        
        # Normalizasyon
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        print(f"\nðŸ“Š Veri BoyutlarÄ±:")
        print(f"   EÄŸitim: {len(X_train)} | Test: {len(X_test)}")
        print(f"   Ã–zellikler: {feature_cols}")
        
        # Binary classification (yÃ¶n tahmini)
        if 'Direction' in target:
            models = {
                'Logistic Regression': LogisticRegression(random_state=42),
                'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
                'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
            }
            
            results = {}
            best_model = None
            best_acc = 0
            
            print("\nðŸŽ¯ Model PerformanslarÄ±:")
            print("   " + "-" * 50)
            
            for name, model in models.items():
                # EÄŸit
                model.fit(X_train_scaled, y_train)
                
                # Tahmin
                y_pred = model.predict(X_test_scaled)
                
                # Metrikler
                acc = accuracy_score(y_test, y_pred)
                
                # Cross-validation (zaman serisi iÃ§in Ã¶zel split)
                tscv = TimeSeriesSplit(n_splits=5)
                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=tscv)
                
                results[name] = {
                    'accuracy': acc,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std()
                }
                
                print(f"   {name}:")
                print(f"      Test Accuracy: {acc:.4f}")
                print(f"      CV Accuracy: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
                
                if acc > best_acc:
                    best_acc = acc
                    best_model = (name, model)
            
            # En iyi model detaylarÄ±
            print(f"\nðŸ† En Ä°yi Model: {best_model[0]} (Accuracy: {best_acc:.4f})")
            
            # Confusion matrix
            y_pred_best = best_model[1].predict(X_test_scaled)
            cm = confusion_matrix(y_test, y_pred_best)
            
            fig, ax = plt.subplots(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                       xticklabels=['DÃ¼ÅŸÃ¼ÅŸ', 'YÃ¼kseliÅŸ'],
                       yticklabels=['DÃ¼ÅŸÃ¼ÅŸ', 'YÃ¼kseliÅŸ'])
            ax.set_xlabel('Tahmin')
            ax.set_ylabel('GerÃ§ek')
            ax.set_title(f'Confusion Matrix - {best_model[0]}', fontsize=14, fontweight='bold')
            plt.tight_layout()
            plt.savefig(os.path.join(RESULTS_DIR, 'confusion_matrix.png'), dpi=150)
            plt.close()
            
            # Feature importance (Random Forest iÃ§in)
            if isinstance(best_model[1], (RandomForestClassifier, GradientBoostingClassifier)):
                importances = best_model[1].feature_importances_
                feat_imp = pd.DataFrame({
                    'feature': feature_cols,
                    'importance': importances
                }).sort_values('importance', ascending=False)
                
                print("\nðŸ“Š Ã–zellik Ã–nemleri:")
                for _, row in feat_imp.iterrows():
                    print(f"   {row['feature']}: {row['importance']:.4f}")
                
                # GÃ¶rselleÅŸtir
                fig, ax = plt.subplots(figsize=(10, 6))
                sns.barplot(data=feat_imp, x='importance', y='feature', palette='viridis', ax=ax)
                ax.set_title('Ã–zellik Ã–nemleri (Feature Importance)', fontsize=14, fontweight='bold')
                ax.set_xlabel('Ã–nem')
                plt.tight_layout()
                plt.savefig(os.path.join(RESULTS_DIR, 'feature_importance.png'), dpi=150)
                plt.close()
            
            # Classification report
            print("\nðŸ“‹ SÄ±nÄ±flandÄ±rma Raporu:")
            print(classification_report(y_test, y_pred_best, 
                                       target_names=['DÃ¼ÅŸÃ¼ÅŸ', 'YÃ¼kseliÅŸ']))
            
            self.model_results['prediction'] = results
            return results
        
        else:
            # Regression (sÃ¼rekli deÄŸer tahmini)
            model = LinearRegression()
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)
            
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            
            print(f"\nðŸ“Š Regresyon SonuÃ§larÄ±:")
            print(f"   MSE: {mse:.4f}")
            print(f"   RÂ²: {r2:.4f}")
            
            return {'mse': mse, 'r2': r2}
    
    def event_study(self, threshold: float = 0.3) -> pd.DataFrame:
        """
        AÅŸÄ±rÄ± pozitif/negatif sentiment gÃ¼nlerinde ne oluyor?
        Event study yaklaÅŸÄ±mÄ±.
        """
        if self.merged_data is None:
            return pd.DataFrame()
        
        print("\n" + "=" * 60)
        print("ðŸ“Š EVENT STUDY ANALÄ°ZÄ°")
        print("=" * 60)
        
        df = self.merged_data
        
        # AÅŸÄ±rÄ± pozitif gÃ¼nler
        positive_events = df[df['avg_sentiment'] > threshold]
        # AÅŸÄ±rÄ± negatif gÃ¼nler
        negative_events = df[df['avg_sentiment'] < -threshold]
        
        print(f"\nðŸ” Threshold: Â±{threshold}")
        print(f"   AÅŸÄ±rÄ± pozitif gÃ¼nler: {len(positive_events)}")
        print(f"   AÅŸÄ±rÄ± negatif gÃ¼nler: {len(negative_events)}")
        
        if len(positive_events) > 0:
            print(f"\n   ðŸ“ˆ Pozitif GÃ¼nlerde Ortalama Ertesi GÃ¼n Getiri: "
                  f"{positive_events['Next_Day_Return'].mean():.4f}%")
        
        if len(negative_events) > 0:
            print(f"   ðŸ“‰ Negatif GÃ¼nlerde Ortalama Ertesi GÃ¼n Getiri: "
                  f"{negative_events['Next_Day_Return'].mean():.4f}%")
        
        # Normal gÃ¼nler
        normal_days = df[(df['avg_sentiment'] >= -threshold) & 
                         (df['avg_sentiment'] <= threshold)]
        print(f"   âž– Normal GÃ¼nlerde Ortalama Ertesi GÃ¼n Getiri: "
              f"{normal_days['Next_Day_Return'].mean():.4f}%")
        
        # GÃ¶rselleÅŸtir
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # Sol: Sentiment daÄŸÄ±lÄ±mÄ±
        ax1 = axes[0]
        ax1.hist(df['avg_sentiment'], bins=30, color='steelblue', edgecolor='white', alpha=0.7)
        ax1.axvline(threshold, color='green', linestyle='--', label=f'Pozitif threshold ({threshold})')
        ax1.axvline(-threshold, color='red', linestyle='--', label=f'Negatif threshold ({-threshold})')
        ax1.set_xlabel('GÃ¼nlÃ¼k Sentiment Skoru')
        ax1.set_ylabel('GÃ¼n SayÄ±sÄ±')
        ax1.set_title('Sentiment DaÄŸÄ±lÄ±mÄ±', fontweight='bold')
        ax1.legend()
        
        # SaÄŸ: Scatter plot
        ax2 = axes[1]
        colors = ['red' if s < -threshold else 'green' if s > threshold else 'gray' 
                  for s in df['avg_sentiment']]
        ax2.scatter(df['avg_sentiment'], df['Next_Day_Return'], c=colors, alpha=0.6, s=30)
        ax2.set_xlabel('Sentiment Skoru')
        ax2.set_ylabel('Ertesi GÃ¼n Getiri (%)')
        ax2.set_title('Sentiment vs Ertesi GÃ¼n Getiri', fontweight='bold')
        ax2.axhline(0, color='black', linestyle='-', linewidth=0.5)
        ax2.axvline(0, color='black', linestyle='-', linewidth=0.5)
        
        plt.tight_layout()
        plt.savefig(os.path.join(RESULTS_DIR, 'event_study.png'), dpi=150)
        plt.close()
        
        return df
    
    def time_series_plot(self):
        """
        Sentiment ve fiyat zaman serisi grafiÄŸi.
        """
        if self.merged_data is None:
            return
        
        df = self.merged_data.copy()
        df['date'] = pd.to_datetime(df['date'])
        
        fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)
        
        # 1. Fiyat grafiÄŸi
        ax1 = axes[0]
        ax1.plot(df['date'], df['Close'], color='navy', linewidth=1.2)
        ax1.set_ylabel('BIST100 KapanÄ±ÅŸ', fontsize=11)
        ax1.set_title('BIST100 ve Sentiment Zaman Serisi', fontsize=14, fontweight='bold')
        ax1.fill_between(df['date'], df['Close'], alpha=0.3, color='navy')
        
        # 2. Sentiment grafiÄŸi
        ax2 = axes[1]
        ax2.bar(df['date'], df['avg_sentiment'], 
               color=['green' if x > 0 else 'red' for x in df['avg_sentiment']], 
               alpha=0.7, width=1)
        ax2.axhline(0, color='black', linestyle='-', linewidth=0.5)
        ax2.set_ylabel('GÃ¼nlÃ¼k Sentiment', fontsize=11)
        ax2.fill_between(df['date'], df['sentiment_ma5'], alpha=0.3, color='blue', label='5-gÃ¼n MA')
        ax2.legend()
        
        # 3. Haber sayÄ±sÄ±
        ax3 = axes[2]
        ax3.bar(df['date'], df['news_count'], color='purple', alpha=0.6, width=1)
        ax3.set_ylabel('Haber SayÄ±sÄ±', fontsize=11)
        ax3.set_xlabel('Tarih', fontsize=11)
        
        plt.tight_layout()
        plt.savefig(os.path.join(RESULTS_DIR, 'time_series.png'), dpi=150)
        plt.close()
        
        print(f"ðŸ“Š Zaman serisi grafiÄŸi kaydedildi: {RESULTS_DIR}/time_series.png")
    
    def generate_report(self) -> str:
        """
        Analiz sonuÃ§larÄ±nÄ±n Ã¶zetini oluÅŸturur.
        """
        report = []
        report.append("=" * 60)
        report.append("ðŸ“Š BIST100 SENTIMENT ANALÄ°ZÄ° RAPORU")
        report.append("=" * 60)
        
        if self.merged_data is not None:
            df = self.merged_data
            report.append(f"\nðŸ“… Veri AralÄ±ÄŸÄ±: {df['date'].min()} - {df['date'].max()}")
            report.append(f"ðŸ“ˆ Toplam GÃ¼n SayÄ±sÄ±: {len(df)}")
            report.append(f"ðŸ“° Toplam Haber SayÄ±sÄ±: {df['news_count'].sum():.0f}")
        
        if 'correlation' in self.model_results:
            report.append("\n--- KORELASYON SONUÃ‡LARI ---")
            for k, v in self.model_results['correlation'].items():
                report.append(f"   {k}: {v:.4f}")
        
        if 'prediction' in self.model_results:
            report.append("\n--- TAHMÄ°N MODELÄ° SONUÃ‡LARI ---")
            for model, metrics in self.model_results['prediction'].items():
                report.append(f"   {model}: Accuracy={metrics['accuracy']:.4f}")
        
        report_text = "\n".join(report)
        
        # Dosyaya kaydet
        report_file = os.path.join(RESULTS_DIR, "analysis_report.txt")
        with open(report_file, 'w') as f:
            f.write(report_text)
        
        print(report_text)
        print(f"\nðŸ’¾ Rapor kaydedildi: {report_file}")
        
        return report_text


def run_full_analysis():
    """
    Tam analiz pipeline'Ä± Ã§alÄ±ÅŸtÄ±rÄ±r.
    """
    analyzer = SentimentPriceAnalyzer()
    
    # 1. Verileri yÃ¼kle ve birleÅŸtir
    analyzer.load_and_merge_data()
    
    if analyzer.merged_data is None or len(analyzer.merged_data) < 10:
        print("âŒ Yeterli veri yok! Ã–nce haber ve fiyat verilerini Ã§ekin.")
        return None
    
    # 2. Korelasyon analizi
    analyzer.correlation_analysis()
    
    # 3. Granger nedensellik
    analyzer.granger_causality_test()
    
    # 4. Tahmin modeli
    analyzer.train_prediction_model()
    
    # 5. Event study
    analyzer.event_study()
    
    # 6. Zaman serisi grafiÄŸi
    analyzer.time_series_plot()
    
    # 7. Rapor oluÅŸtur
    analyzer.generate_report()
    
    return analyzer


if __name__ == "__main__":
    run_full_analysis()

